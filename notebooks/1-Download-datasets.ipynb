{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Planet data, crop it, and also download associated hand drawn classifications. Also download co-incident OPERA DSWx data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "This notebook expects a co-located '.env' file containing a planet API key in the format \n",
    "> PLANET_API_KEY='[key]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gis imports\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "from rasterio.plot import show\n",
    "from rasterio.warp import transform_bounds\n",
    "\n",
    "# planet api imports\n",
    "from planet import api\n",
    "from planet.api import downloader\n",
    "from planet.api.downloader import create\n",
    "\n",
    "# misc imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import dotenv_values\n",
    "from tools import addImageCalc\n",
    "from pathlib import Path\n",
    "\n",
    "# data science imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# aws imports\n",
    "import boto3\n",
    "from botocore.handlers import disable_signing\n",
    "\n",
    "# pySTAC imports\n",
    "from pystac_client import Client\n",
    "\n",
    "os.environ[\"AWS_NO_SIGN_REQUEST\"] = \"YES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chip IDs that we will test in this notebook\n",
    "# these should be chip_ids for which hand-classifications were made\n",
    "\n",
    "df = pd.read_csv('../data/validation_table.csv')\n",
    "chip_ids = df.site_name.unique()\n",
    "\n",
    "print(chip_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planet data downloader client\n",
    "PLANET_API_KEY = dotenv_values()['PLANET_API_KEY']\n",
    "\n",
    "# setup AWS boto3 client\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.meta.events.register('choose-signer.s3.*', disable_signing)\n",
    "s3.meta.client.meta.events.register('choose-signer.s3.*', disable_signing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_planet_imagery(chip_id):\n",
    "    \"\"\" \n",
    "    Given a Planet image id, download associated planetscope imagery. \n",
    "    \n",
    "    If a file already exists at the download location, this function will not overwrite it\n",
    "    \"\"\"\n",
    "    client = api.ClientV1(api_key=PLANET_API_KEY)\n",
    "    planet_data_downloader = downloader.create(client)\n",
    "\n",
    "    df_images = gpd.read_file('s3://opera-calval-database-dswx/image.geojson')\n",
    "    df_images.dropna(inplace=True)\n",
    "    df_images[df_images.site_name == chip_id]\n",
    "\n",
    "    temp = df_images[['image_name', 'site_name']]\n",
    "    df_site2image = temp.set_index('site_name')\n",
    "    df_image2site = temp.set_index('image_name')\n",
    "    df_site2image.head()\n",
    "\n",
    "    PLANET_ID = df_images[df_images.site_name == chip_id].image_name.values[0]\n",
    "    data_dir = Path(f'../data/{PLANET_ID}/')\n",
    "    data_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # check if planet data has already been downloaded\n",
    "    n_planet_images = len(list(data_dir.glob(f\"{PLANET_ID}_*AnalyticMS*.tif\")))\n",
    "\n",
    "    if n_planet_images == 0:\n",
    "        ITEM_TYPE = 'PSScene'\n",
    "        ASSET_TYPES = ['ortho_analytic_8b_sr', \n",
    "                    'ortho_analytic_8b_xml']\n",
    "\n",
    "        req = client.get_item(ITEM_TYPE, PLANET_ID)\n",
    "        # activate assets\n",
    "        resp = req.get()\n",
    "        if 'ortho_analytic_8b_sr' not in resp['assets']:\n",
    "            # download 4b_sr if 8b_sr is not available\n",
    "            ASSET_TYPES = [ 'ortho_analytic_4b_sr', 'ortho_analytic_4b_xml']\n",
    "        \n",
    "        items_to_download = [resp] * len(ASSET_TYPES)\n",
    "        resp_ac = planet_data_downloader.activate(iter(items_to_download), ASSET_TYPES)\n",
    "\n",
    "        resp_dl = planet_data_downloader.download(iter(items_to_download), ASSET_TYPES, str(data_dir))\n",
    "    else:\n",
    "        print(f\"Planet images for chip id {chip_id} already exist at {data_dir}. Delete the files to re-download\")\n",
    "\n",
    "    return PLANET_ID\n",
    "        \n",
    "\n",
    "def crop_planet_imagery(PLANET_ID):\n",
    "    \"\"\"\n",
    "    For a given site_name / planet_id, validation data was generated over a cropped sub-region. This function reads\n",
    "    the geometry of the cropped region and writes out the cropped image to a separate file.\n",
    "\n",
    "    If a file already exists at the output location, this function will not overwrite it.\n",
    "    \"\"\"\n",
    "    df_images = gpd.read_file('s3://opera-calval-database-dswx/image.geojson')\n",
    "    df_images.dropna(inplace=True)\n",
    "    df_site = gpd.read_file('s3://opera-calval-database-dswx/site.geojson')\n",
    "    df_site.dropna(inplace=True)\n",
    "\n",
    "    col_list = list(df_images.keys())\n",
    "    col_list.remove('geometry')\n",
    "    df_temp = df_images[col_list]\n",
    "    df_chips = pd.merge(df_site, df_temp , on='site_name', how='left')\n",
    "    temp = df_chips[['image_name', 'site_name']]\n",
    "    df_site2image = temp.set_index('site_name')\n",
    "    df_image2site = temp.set_index('image_name')\n",
    "\n",
    "    data_dir = Path(f'../data/{PLANET_ID}/')\n",
    "    data_dir.mkdir(exist_ok=True, parents=True)\n",
    "    cropped_dir = Path(f'../data/planet_images_cropped/{PLANET_ID}/')\n",
    "    cropped_file = list(cropped_dir.glob(f\"cropped_{PLANET_ID}*.tif\"))\n",
    "\n",
    "    # proceed with cropping planet image only if it hasn't been done already\n",
    "    if len(cropped_file) == 0:\n",
    "        cropped_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        n = len(PLANET_ID)\n",
    "        planet_images = list(data_dir.glob('*.tif'))\n",
    "        planet_image_path = list(filter(lambda x: x.name[:n] == PLANET_ID, planet_images))[0]\n",
    "\n",
    "        with rasterio.open(planet_image_path) as ds:\n",
    "            planet_crs = ds.crs\n",
    "            planet_profile = ds.profile\n",
    "        \n",
    "        df_chip = df_chips[df_chips.image_name == PLANET_ID]\n",
    "\n",
    "        # 500 meter buffer\n",
    "        df_chip_utm = df_chip.to_crs(planet_crs).buffer(500, join_style=2)\n",
    "\n",
    "        with rasterio.open(planet_image_path) as src:\n",
    "            out_image, out_transform = rasterio.mask.mask(src, df_chip_utm.geometry, crop=True)\n",
    "            out_meta = src.meta\n",
    "\n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "                \"height\": out_image.shape[1],\n",
    "                \"width\": out_image.shape[2],\n",
    "                \"transform\": out_transform,\n",
    "                \"compress\": \"lzw\"})\n",
    "\n",
    "        with rasterio.open(cropped_dir / f'cropped_{PLANET_ID}.tif', \"w\", **out_meta) as dest:\n",
    "            dest.write(out_image)\n",
    "    else:\n",
    "        print(f\"Cropped image for planet id {PLANET_ID} already exist at {cropped_dir}. Delete the file to re-download\")\n",
    "\n",
    "    return cropped_dir\n",
    "\n",
    "def download_validation_data(PLANET_ID, cropped_dir):\n",
    "    \"\"\"\n",
    "    For a given planet_id, download the validation data from the OPERA Cal-Val S3 bucket. The location of the validation \n",
    "    data is obtained from the validation_table.csv file.\n",
    "\n",
    "    If a file already exists at the download location, this function will not overwrite it.\n",
    "    \"\"\"\n",
    "    classification_file = list(cropped_dir.glob(f\"classification_{PLANET_ID}*.tif\"))\n",
    "    if len(classification_file) == 0:\n",
    "\n",
    "        imageTable = gpd.read_file('s3://opera-calval-database-dswx/image.geojson')\n",
    "        image_calcs = gpd.read_file('s3://opera-calval-database-dswx/image_calc.geojson')\n",
    "\n",
    "        download_dir = Path(f'../data/planet_images_cropped/{PLANET_ID}').absolute()\n",
    "        download_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        def downloadImage_calc(row,download_dir):\n",
    "            bucket = row.bucket.iloc[0]\n",
    "            keys = row.s3_keys.iloc[0]\n",
    "            keys = keys.split(',')\n",
    "            for key in keys:\n",
    "                filename = key.split('/')[-1]\n",
    "                response = s3_client.download_file(bucket,\n",
    "                                                key,\n",
    "                                                str(download_dir / filename))\n",
    "\n",
    "        search = image_calcs[image_calcs.image_name == PLANET_ID]\n",
    "\n",
    "        try:\n",
    "            search_iter = search[search.version==search['version'].max()]\n",
    "            search_iter = search_iter.iloc[[0]]\n",
    "        except IndexError:\n",
    "            search_iter = search[search.upload_date.values==search.upload_date.values.max()]\n",
    "            imagecalc_row = search_iter\n",
    "\n",
    "        imagecalc_row = search_iter\n",
    "        image_calc_name = imagecalc_row.image_calc_name.iloc[0]\n",
    "        version = imagecalc_row.version.iloc[0]\n",
    "        imagecalc_row.to_file(download_dir / f'metadata_{PLANET_ID}_v{version}.geojson', driver='GeoJSON')\n",
    "        downloadImage_calc(imagecalc_row,download_dir)\n",
    "    else:\n",
    "        print(f\"Validation data for planet id {PLANET_ID} already exist at {cropped_dir}. Delete the files to re-download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to process DSWx data\n",
    "def get_fmask_url(hls_id: str) -> str:\n",
    "    BANDS = ['Fmask']\n",
    "    STAC_URL = 'https://cmr.earthdata.nasa.gov/stac'\n",
    "\n",
    "    api = Client.open(f'{STAC_URL}/LPCLOUD/')\n",
    "    hls_collections = ['HLSL30.v2.0', 'HLSS30.v2.0']\n",
    "\n",
    "    search_params = {\"collections\": hls_collections, \"ids\": hls_id}\n",
    "    search_hls = api.search(**search_params)\n",
    "\n",
    "    assert(search_hls.matched() == 1)\n",
    "    hls_collection = list(search_hls.get_all_items())\n",
    "    metadata = hls_collection[0].to_dict()\n",
    "    urls = [metadata['assets'].get(band, {'href': ''})['href'] for band in BANDS]\n",
    "    return urls[0]\n",
    "\n",
    "def download_dswx_data(planet_id):\n",
    "    save_path = Path('../data') / planet_id\n",
    "    df = pd.read_csv('../data/validation_table.csv')\n",
    "    row = df[df['planet_id'] == planet_id]\n",
    "  \n",
    "    dswx_file = row.dswx_urls.values[0].split()[0]\n",
    "    fmask_file = get_fmask_url(row.hls_id.values[0])\n",
    "    conf_file = row.dswx_urls.values[0].split()[2]\n",
    "\n",
    "    os.system(f\"wget {dswx_file} -q -nc -P {save_path}\")\n",
    "    os.system(f\"wget {fmask_file} -q -nc -P {save_path}\")\n",
    "    os.system(f\"wget {conf_file} -q -nc -P {save_path}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(chip_id):\n",
    "    \n",
    "    # download planet data\n",
    "    planet_id = download_planet_imagery(chip_id)\n",
    "    \n",
    "    # crop planet data\n",
    "    cropped_dir = crop_planet_imagery(planet_id)\n",
    "    \n",
    "    # download validation data\n",
    "    download_validation_data(planet_id, cropped_dir)\n",
    "\n",
    "    # download overlapping DSWx tile\n",
    "    download_dswx_data(planet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all chips\n",
    "_ = list(map(main, chip_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('expand-validation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a751338cf89ec1649cdf743d3dc7fe23ec82d22a9f9be14ff02c9be8441ee2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
