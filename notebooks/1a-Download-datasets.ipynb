{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook demonstrates how to download the OPERA DSWx-HLS validation dataset\n",
    "In particular, the notebook will do the following things:\n",
    "- Download Planet data used to validate the DSWx-HLS data product and crop it to the appropriate extent\n",
    "- For each cropped Planet scene, download the hand drawn binary water classification \n",
    "- For each Planet scene, download co-incident OPERA DSWx-HLS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### NOTES BEFORE RUNNING\n",
    "1. This notebook expects a co-located '.env' file containing a planet API key in the format \n",
    "> PLANET_API_KEY='[key]'\n",
    "\n",
    "2. This notebook will take a *significant* amount of time to execute (~6-12 hours or more) due to the large volume of data that needs to be downloaded\n",
    "3. If the notebook fails at the data download stage, re-run the cell. The notebook will resume download from the last successfully downloaded scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gis imports\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "from rasterio.plot import show\n",
    "from rasterio.warp import transform_bounds\n",
    "\n",
    "# planet api imports\n",
    "from planet import api\n",
    "from planet.api import downloader\n",
    "from planet.api.downloader import create\n",
    "\n",
    "# misc imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import dotenv_values\n",
    "from tools import addImageCalc\n",
    "from pathlib import Path\n",
    "\n",
    "# data science imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# aws imports\n",
    "import boto3\n",
    "from botocore.handlers import disable_signing\n",
    "\n",
    "# pySTAC imports\n",
    "from pystac_client import Client\n",
    "\n",
    "os.environ[\"AWS_NO_SIGN_REQUEST\"] = \"YES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chip IDs that we will test in this notebook\n",
    "# these should be chip_ids for which hand-classifications were made\n",
    "\n",
    "df = pd.read_csv('../data/validation_table.csv')\n",
    "chip_ids = df.site_name.unique()\n",
    "print(chip_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planet data downloader client\n",
    "PLANET_API_KEY = dotenv_values()['PLANET_API_KEY']\n",
    "\n",
    "# setup AWS boto3 client\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.meta.events.register('choose-signer.s3.*', disable_signing)\n",
    "s3.meta.client.meta.events.register('choose-signer.s3.*', disable_signing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageTable = gpd.read_file('s3://opera-calval-database-dswx/image.geojson')\n",
    "imageTable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_calcs = gpd.read_file('s3://opera-calval-database-dswx/image_calc.geojson')\n",
    "image_calcs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images = gpd.read_file('s3://opera-calval-database-dswx/image.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images = gpd.read_file('s3://opera-calval-database-dswx/site.geojson')\n",
    "df_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "# Given a chip_id, download corresponding Planet imagery\n",
    "def download_planet_imagery(chip_id):\n",
    "    \"\"\" \n",
    "    Given a Planet image id, download associated planetscope imagery. \n",
    "    \n",
    "    If a file already exists at the download location, this function will not overwrite it\n",
    "    \"\"\"\n",
    "    client = api.ClientV1(api_key=PLANET_API_KEY)\n",
    "    planet_data_downloader = downloader.create(client)\n",
    "\n",
    "    df_images = gpd.read_file('s3://opera-calval-database-dswx/image.geojson')\n",
    "    df_images.dropna(inplace=True)\n",
    "    df_images[df_images.site_name == chip_id]\n",
    "\n",
    "    temp = df_images[['image_name', 'site_name']]\n",
    "    df_site2image = temp.set_index('site_name')\n",
    "    df_image2site = temp.set_index('image_name')\n",
    "    df_site2image.head()\n",
    "\n",
    "    PLANET_ID = df_images[df_images.site_name == chip_id].image_name.values[0]\n",
    "    data_dir = Path(f'../data/{PLANET_ID}/')\n",
    "    data_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # check if planet data has already been downloaded\n",
    "    n_planet_images = len(list(data_dir.glob(f\"{PLANET_ID}_*AnalyticMS*.tif\")))\n",
    "\n",
    "    if n_planet_images == 0:\n",
    "        ITEM_TYPE = 'PSScene'\n",
    "        ASSET_TYPES = ['ortho_analytic_8b_sr', \n",
    "                    'ortho_analytic_8b_xml']\n",
    "\n",
    "        req = client.get_item(ITEM_TYPE, PLANET_ID)\n",
    "        # activate assets\n",
    "        resp = req.get()\n",
    "        if 'ortho_analytic_8b_sr' not in resp['assets']:\n",
    "            # download 4b_sr if 8b_sr is not available\n",
    "            ASSET_TYPES = [ 'ortho_analytic_4b_sr', 'ortho_analytic_4b_xml']\n",
    "        \n",
    "        items_to_download = [resp] * len(ASSET_TYPES)\n",
    "        resp_ac = planet_data_downloader.activate(iter(items_to_download), ASSET_TYPES)\n",
    "\n",
    "        resp_dl = planet_data_downloader.download(iter(items_to_download), ASSET_TYPES, str(data_dir))\n",
    "    else:\n",
    "        print(f\"Planet images for chip id {chip_id} already exist at {data_dir}. Delete the files to re-download\")\n",
    "\n",
    "    return PLANET_ID\n",
    "        \n",
    "# Crop downloaded planet imagery\n",
    "def crop_planet_imagery(PLANET_ID):\n",
    "    \"\"\"\n",
    "    For a given site_name / planet_id, validation data was generated over a cropped sub-region. This function reads\n",
    "    the geometry of the cropped region and writes out the cropped image to a separate file.\n",
    "\n",
    "    If a file already exists at the output location, this function will not overwrite it.\n",
    "    \"\"\"\n",
    "    df_images = gpd.read_file('s3://opera-calval-database-dswx/image.geojson')\n",
    "    df_images.dropna(inplace=True)\n",
    "    df_site = gpd.read_file('s3://opera-calval-database-dswx/site.geojson')\n",
    "    df_site.dropna(inplace=True)\n",
    "\n",
    "    col_list = list(df_images.keys())\n",
    "    col_list.remove('geometry')\n",
    "    df_temp = df_images[col_list]\n",
    "    df_chips = pd.merge(df_site, df_temp , on='site_name', how='left')\n",
    "    temp = df_chips[['image_name', 'site_name']]\n",
    "    df_site2image = temp.set_index('site_name')\n",
    "    df_image2site = temp.set_index('image_name')\n",
    "\n",
    "    data_dir = Path(f'../data/{PLANET_ID}/')\n",
    "    data_dir.mkdir(exist_ok=True, parents=True)\n",
    "    cropped_dir = Path(f'../data/planet_images_cropped/{PLANET_ID}/')\n",
    "    cropped_file = list(cropped_dir.glob(f\"cropped_{PLANET_ID}*.tif\"))\n",
    "\n",
    "    # proceed with cropping planet image only if it hasn't been done already\n",
    "    if len(cropped_file) == 0:\n",
    "        cropped_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        planet_image_path = list(data_dir.glob(f'{PLANET_ID}*AnalyticMS*.tif'))[0]\n",
    "        with rasterio.open(planet_image_path) as ds:\n",
    "            planet_crs = ds.crs\n",
    "            planet_profile = ds.profile\n",
    "        \n",
    "        if PLANET_ID not in ['20210916_010848_94_2407', '20210924_133812_95_2420', '20210925_072712_16_2254', '20211028_144231_39_227b', '20211030_142613_41_227b']:\n",
    "            df_chip = df_chips[df_chips.image_name == PLANET_ID]\n",
    "\n",
    "            # 500 meter buffer\n",
    "            df_chip_utm = df_chip.to_crs(planet_crs).buffer(500, join_style=2)\n",
    "        else:\n",
    "            # For Planet ID == 20210916_010848_94_2407, the cropped geometry specified in s3://opera-calval-database-dswx/site.geojson is incorrect\n",
    "            # The correct geometry is present in the ../data/validation_table.csv file\n",
    "            df = pd.read_csv('../data/validation_table.csv')\n",
    "            df = gpd.GeoDataFrame(df.loc[:, [c for c in df.columns if c != \"geometry\"]], geometry=gpd.GeoSeries.from_wkt(df[\"geometry\"]), crs=\"epsg:4326\")\n",
    "            df_chip_utm = df[df['planet_id'] == PLANET_ID].to_crs(planet_crs)\n",
    "            \n",
    "        with rasterio.open(planet_image_path) as src:\n",
    "            out_image, out_transform = rasterio.mask.mask(src, df_chip_utm.geometry, crop=True)\n",
    "            out_meta = src.meta\n",
    "\n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "                \"height\": out_image.shape[1],\n",
    "                \"width\": out_image.shape[2],\n",
    "                \"transform\": out_transform,\n",
    "                \"compress\": \"lzw\"})\n",
    "\n",
    "        with rasterio.open(cropped_dir / f'cropped_{PLANET_ID}.tif', \"w\", **out_meta) as dest:\n",
    "            dest.write(out_image)\n",
    "    else:\n",
    "        print(f\"Cropped image for planet id {PLANET_ID} already exist at {cropped_dir}. Delete the file to re-download\")\n",
    "\n",
    "    return cropped_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(chip_id):\n",
    "    \n",
    "    # download planet data\n",
    "    planet_id = download_planet_imagery(chip_id)\n",
    "    \n",
    "    # crop planet data\n",
    "    cropped_dir = crop_planet_imagery(planet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all chips\n",
    "_ = list(map(main, chip_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With data downloaded, create and save a database of all the relevant files needed to expand the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/validation_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a table containing DSWx, Fmask, and RF classification\n",
    "def return_dswx_path(planet_id):\n",
    "    data_path = Path('../data')/planet_id/'dswx'\n",
    "    return \",\".join([str(f) for f in list(data_path.glob(\"OPERA_L3_DSWx*_B01_WTR.tif\"))])\n",
    "\n",
    "def return_conf_path(planet_id):\n",
    "    data_path = Path('../data')/planet_id/'dswx'\n",
    "    return \",\".join([str(f) for f in list(data_path.glob(\"OPERA_L3_DSWx*_B03_CONF.tif\"))])\n",
    "\n",
    "def return_val_path(planet_id):\n",
    "    data_path = Path('../data')/'planet_images_cropped'/planet_id\n",
    "    return \",\".join([str(f) for f in list(data_path.glob(f\"site_name-*-classified_planet-*{planet_id}*.tif\"))])\n",
    "\n",
    "df['dswx_files'] = df['planet_id'].map(return_dswx_path)\n",
    "df['conf_files'] = df['planet_id'].map(return_conf_path)\n",
    "df['val_files'] = df['planet_id'].map(return_val_path)\n",
    "\n",
    "# write out table for future use\n",
    "df_new = df[['site_name', 'planet_id', 'dswx_files', 'conf_files', 'val_files']]\n",
    "df_new.to_csv('../data/new_validation_table.csv', index=None)\n",
    "\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upon successful end-to-end execution of this notebook\n",
    "1. The `data` folder should be populated by folders containing individual Planet scenes (geotif and metadata), along with the associated OPERA DSWx-HLS data and corresponding HLS Fmask data\n",
    "2. The `data` folder will also contain a folder named `planet_images_cropped` which will contain folders for the same 52 Planet scenes, with the cropped Planet imagery and associated hand labeled water mask.\n",
    "\n",
    "For example, for the Planet scene `20210903_150800_60_2458`:\n",
    "```\n",
    "    .\n",
    "    ├── data\n",
    "    │   ├─ 20210903_150800_60_2458\n",
    "    │   │  ├─ 20210903_150800_60_2458_3B_AnalyticMS_8b_metadata.xml\n",
    "    │   │  ├─ 20210903_150800_60_2458_3B_AnalyticMS_SR_8b.tif\n",
    "    │   │  ├─ dswx\n",
    "    │   │  │  ├─ OPERA_L3_DSWx-HLS_T18UXG_20210902T154154Z_20230906T035356Z_L8_30_v1.1_B01_WTR.tif\n",
    "    │   │  │  ├─ OPERA_L3_DSWx-HLS_T18UXG_20210902T154154Z_20230906T035356Z_L8_30_v1.1_B03_CONF.tif\n",
    "    │   │  │  └─ ... \n",
    "    │   ├─ ...\n",
    "    │   ├─ planet_images_cropped\n",
    "    │   │  ├─ 20210903_150800_60_2458\n",
    "    │   │  │  ├─ site_name-4_21-classified_planet-20210903_150800_60_2458.tif\n",
    "    │   │  │  ├─ cropped_20210903_150800_60_2458.tif\n",
    "    │   │  │  └─ Site-4_21-metadata.json\n",
    "    │   │  └─ ...\n",
    "    │   ├─ new_validation_table.csv \n",
    "    │   └─ validation_table.csv \n",
    "    ├── notebooks\n",
    "    │   └─ ...\n",
    "    ├── environment.yml\n",
    "    └── README.md       \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit ('expand-validation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a751338cf89ec1649cdf743d3dc7fe23ec82d22a9f9be14ff02c9be8441ee2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
