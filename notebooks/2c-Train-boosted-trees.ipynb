{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce two improvements to generate better water mask inferences from Planet imagery\n",
    "- First, we train a Histogram-based Gradient Boosting Classification Tree instead of a Random Forest model\n",
    "- Second, we generate segments using a preprocessed NIR channel PlanetScope image (compared to using the `[green, nir, ndwi]` stack in previous notebooks)\n",
    "\n",
    "The remaining training and inference steps remain the same. At the end of our notebook, we save our model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIS imports\n",
    "import rasterio\n",
    "\n",
    "# scikit and numpy imports\n",
    "import numpy as np\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from skimage.segmentation import felzenszwalb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from skimage import filters, exposure\n",
    "from skimage.morphology import square\n",
    "\n",
    "# misc imports\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import joblib\n",
    "from joblib import dump\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# local imports\n",
    "from tools import (get_superpixel_stds_as_features, get_superpixel_means_as_features, \n",
    "                    get_array_from_features, reproject_arr_to_match_profile, get_segment_sizes)\n",
    "from rf_funcs import calc_ndwi, calc_ndvi, return_grn_indices, return_img_bands, return_reflectance_coeffs\n",
    "\n",
    "# for repeatability\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to re-train the model\n",
    "RETRAIN_MODEL=True\n",
    "EVALUATE_MODEL=True # Split available data and print model performance on test set\n",
    "TEST_SET_SPLIT=0.15 # If evaluating model, specify data split for testing. train split will be 1-TEST_SET_SPLIT\n",
    "UPDATE_DATABASE=True # Set to True to update validation database. When re-calculating DSWx-HLS accuracy, the model outputs generated here will be used\n",
    "\n",
    "# BOOSTED TREES PARAMETERS\n",
    "MODEL_LEARNING_RATE=0.8\n",
    "MODEL_MAX_ITER=300\n",
    "MODEL_L2_REGULARIZATION=0.3\n",
    "\n",
    "# FELZENSZWALB PARAMETERS\n",
    "F_SCALE = 20\n",
    "F_MINSIZE = 20\n",
    "F_SIGMA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the validation database\n",
    "data_path = Path('../data/')\n",
    "val_chips_db = data_path / 'validation_table.csv'\n",
    "val_df = pd.read_csv(val_chips_db)\n",
    "\n",
    "site_names = list(val_df['site_name'])\n",
    "planet_ids = list(val_df['planet_id'])\n",
    "\n",
    "# Extract planet IDs and associated strata\n",
    "site_names_stratified = defaultdict(list)\n",
    "for sn, planet_id in zip(site_names, planet_ids):\n",
    "    site_names_stratified[sn[:2]].append(planet_id)\n",
    "\n",
    "training_sites = []\n",
    "\n",
    "# We can either use 4 chips from each strata, or ALL chips from each strata\n",
    "for key in site_names_stratified.keys():\n",
    "        training_sites.extend(site_names_stratified[key])\n",
    "\n",
    "print(\"# of Training sites: \", len(training_sites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function that takes a Planet image and associated labels and returns training samples to be used by our model\n",
    "def process_one_site(site, denoising_weight=0.):\n",
    "    current_img_path = data_path / site\n",
    "    cropped_img_path = data_path / 'planet_images_cropped' / site\n",
    "    \n",
    "    xml_file = list(current_img_path.glob('*.xml'))[0]\n",
    "    chip = list(cropped_img_path.glob(f'cropped_{site}*.tif'))[0]\n",
    "    classification = list(cropped_img_path.glob(f'site_name-*.tif'))[0] \n",
    "\n",
    "    band_idxs = return_grn_indices(xml_file)\n",
    "    coeffs = return_reflectance_coeffs(xml_file, band_idxs)\n",
    "    chip_img = return_img_bands(chip, band_idxs, denoising_weight=denoising_weight)\n",
    "    with rasterio.open(chip) as src_ds:\n",
    "        ref_profile = src_ds.profile\n",
    "\n",
    "    green = chip_img[0]*coeffs[band_idxs[0]]\n",
    "    red = chip_img[1]*coeffs[band_idxs[1]]\n",
    "    nir = chip_img[2]*coeffs[band_idxs[2]]\n",
    "\n",
    "    nodata_mask = nir == ref_profile['nodata']\n",
    "\n",
    "    ndwi = calc_ndwi(green, nir)\n",
    "    ndvi = calc_ndvi(red, nir)\n",
    "\n",
    "    with rasterio.open(classification) as src_ds:\n",
    "        cl = src_ds.read(1)\n",
    "        cl_profile = src_ds.profile\n",
    "\n",
    "    # some classification extents are not the same as the corresponding planet chip extent\n",
    "    # if they are not the same, reproject the validation data to match the profile of the planet data\n",
    "    if ((ref_profile['transform'] != cl_profile['transform']) | \n",
    "        (ref_profile['width'] != cl_profile['width']) | \n",
    "        (ref_profile['height'] != cl_profile['height'])):\n",
    "\n",
    "        cl, _ = reproject_arr_to_match_profile(cl, cl_profile, ref_profile)\n",
    "        cl = np.squeeze(cl)\n",
    "    \n",
    "    new_img = exposure.adjust_gamma(exposure.equalize_hist(filters.scharr(nir), nbins=64), gamma=20)\n",
    "    new_img[nodata_mask] = np.nan\n",
    "    segments = felzenszwalb(new_img, scale=F_SCALE, min_size=F_MINSIZE, sigma=F_SIGMA)\n",
    "\n",
    "    # create training data that includes other channels as well\n",
    "    img_stack = np.stack([red, nir, green, ndwi, ndvi], axis=-1)     \n",
    "    std_features = get_superpixel_stds_as_features(segments, img_stack)\n",
    "    mean_features = get_superpixel_means_as_features(segments, img_stack)\n",
    "    segment_sizes = get_segment_sizes(segments)\n",
    "\n",
    "    X_temp = np.concatenate([mean_features, std_features, segment_sizes], axis = 1)\n",
    "\n",
    "    # # We have superpixels, we now need to map each of the segments to the associated label\n",
    "    # # A 0 value indicates no label for the segment\n",
    "    class_features_temp = np.zeros((mean_features.shape[0], 1)) + 255\n",
    "\n",
    "    superpixel_labels_for_land = set(np.unique(segments[cl == 0]))\n",
    "    superpixel_labels_for_water = set(np.unique(segments[cl == 1]))\n",
    "\n",
    "    intersecting_labels = superpixel_labels_for_land.intersection(superpixel_labels_for_water)\n",
    "    superpixel_labels_for_land = list(superpixel_labels_for_land - intersecting_labels)\n",
    "    superpixel_labels_for_water = list(superpixel_labels_for_water - intersecting_labels)\n",
    "\n",
    "    class_features_temp[superpixel_labels_for_land] = 0\n",
    "    class_features_temp[superpixel_labels_for_water] = 1\n",
    "\n",
    "    return X_temp, class_features_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each of the training sites - \n",
    "# 1. Read the cropped planet image and the corresponding labeled data\n",
    "# 2. Segment image to generate training samples\n",
    "# 3. Append training samples to list\n",
    "\n",
    "rf_model_folder = data_path / 'trained_model' / 'rf_model'\n",
    "rf_model_folder.mkdir(exist_ok=True, parents=True)\n",
    "model_path = rf_model_folder/\"xgboost_model.joblib\"\n",
    "\n",
    "# Write out model parameters for future reference\n",
    "if RETRAIN_MODEL:\n",
    "    with open(f\"{model_path}.txt\", 'w') as f:\n",
    "        f.write(f\"MODEL_LEARNING_RATE:{MODEL_LEARNING_RATE}\\n\")\n",
    "        f.write(f\"MODEL_MAX_ITER:{MODEL_MAX_ITER}\\n\")\n",
    "        f.write(f\"MODEL_L2_REGULARIZATION:{MODEL_L2_REGULARIZATION}\\n\\n\")\n",
    "\n",
    "        f.write(f\"FELZENSZWALB SCALE:{F_SCALE}\\n\")\n",
    "        f.write(f\"FELZENSZWALB MIN SIZE:{F_MINSIZE}\\n\")\n",
    "        f.write(f\"FELZENSZWALB SIGMA:{F_SIGMA}\\n\")\n",
    "\n",
    "    features_and_labels = list(map(process_one_site, tqdm(training_sites))) \n",
    "    X = np.concatenate([x[0] for x in features_and_labels], axis=0)\n",
    "    class_features = np.concatenate([x[1] for x in features_and_labels], axis=0)\n",
    "\n",
    "    # remove all segments where label is > 1 (no data regions)\n",
    "    valid_idxs = np.squeeze(class_features < 2)\n",
    "    X = X[valid_idxs, :]\n",
    "    class_features = class_features[valid_idxs, :]\n",
    "\n",
    "    print(\"Array lengths: \", X.shape, class_features.shape)\n",
    "\n",
    "    print(\"Beginning model training\")\n",
    "    rf = HistGradientBoostingClassifier(random_state=0, class_weight='balanced',early_stopping=False, max_iter=MODEL_MAX_ITER, learning_rate=MODEL_LEARNING_RATE, l2_regularization=MODEL_L2_REGULARIZATION) \n",
    "\n",
    "    if EVALUATE_MODEL:\n",
    "        X, X_test, class_features, class_features_test = train_test_split(X, class_features, train_size=TEST_SET_SPLIT, random_state=42)\n",
    "\n",
    "    # train model on all of the available data\n",
    "    rf.fit(X, class_features.ravel())\n",
    "    if EVALUATE_MODEL:\n",
    "        # print(\"Model OOB score: \", rf.oob_score_)\n",
    "        y_pred = rf.predict(X_test)\n",
    "        print(\"Precision/Recall/Support: \", score(y_pred, class_features_test))\n",
    "        with open(f\"{model_path}.txt\", 'a') as f:\n",
    "            f.write(f\"\\n\\nPrecision/Recall/Support: {score(y_pred, class_features_test)}\")\n",
    "\n",
    "    # save model weights\n",
    "    dump(rf, model_path)\n",
    "\n",
    "# Let's make inferences on the broader planet images\n",
    "def generate_inference_helper(rf, img:str|Path, xml_file:str|Path, denoising_weight=0.):\n",
    "    band_idxs = return_grn_indices(xml_file)\n",
    "    coeffs = return_reflectance_coeffs(xml_file, band_idxs)\n",
    "    \n",
    "    full_img = return_img_bands(img, band_idxs, denoising_weight=denoising_weight)\n",
    "\n",
    "    green = full_img[0]*coeffs[band_idxs[0]]\n",
    "    red = full_img[1]*coeffs[band_idxs[1]]\n",
    "    nir = full_img[2]*coeffs[band_idxs[2]]\n",
    "\n",
    "    with rasterio.open(img) as ds:\n",
    "        ref_profile = ds.profile\n",
    "\n",
    "    nodata_mask = nir == ref_profile['nodata']\n",
    "\n",
    "    ndwi = calc_ndwi(green, nir)\n",
    "    ndvi = calc_ndvi(red, nir)\n",
    "\n",
    "    print(f\"Starting segmentation for {img}\")\n",
    "    new_img = exposure.adjust_gamma(exposure.equalize_hist(filters.scharr(nir), nbins=64), gamma=20)\n",
    "    new_img[nodata_mask] = np.nan\n",
    "    \n",
    "    segments = felzenszwalb(new_img, scale=F_SCALE, min_size=F_MINSIZE, sigma=F_SIGMA)\n",
    "    \n",
    "    print(f\"Segmentation complete for {img}\")\n",
    "\n",
    "    # for inference we include other channels as well\n",
    "    img_stack = np.stack([red, nir, green, ndwi, ndvi], axis=-1)\n",
    "    std_features = get_superpixel_stds_as_features(segments, img_stack)\n",
    "    mean_features = get_superpixel_means_as_features(segments, img_stack)\n",
    "    segment_sizes = get_segment_sizes(segments)\n",
    "\n",
    "    X = np.concatenate([mean_features, std_features, segment_sizes], axis = 1)\n",
    "    print(f\"Starting inference for {img}\")\n",
    "    y = rf.predict(X)\n",
    "\n",
    "    return get_array_from_features(segments, np.expand_dims(y, axis=1))\n",
    "\n",
    "def generate_inference(planet_id):\n",
    "    \"\"\" \n",
    "    This function takes in a planet_id and generates inferences for the overlapping planet image\n",
    "    \"\"\"\n",
    "    data_path = Path('../data')\n",
    "    \n",
    "    current_img_path = data_path / planet_id\n",
    "    cropped_img_path = data_path / 'planet_images_cropped' / planet_id\n",
    "    xml_file = list(current_img_path.glob('*.xml'))[0]\n",
    "    classification = list(cropped_img_path.glob(f'site_name-*{planet_id}*.tif'))[0]\n",
    "\n",
    "    img = list(current_img_path.glob(f'{planet_id}*.tif'))[0]\n",
    "    \n",
    "    print(\"Test file name:\", img)\n",
    "    assert img.exists(), \"File does not exist!!\"\n",
    "\n",
    "    # make a local copy of the model would help\n",
    "    _ = os.system(f\"cp {model_path} {current_img_path}\")\n",
    "    model_copy = current_img_path / model_path.name\n",
    "\n",
    "    rf = joblib.load(model_copy)\n",
    "\n",
    "    inference = generate_inference_helper(rf, img, xml_file)\n",
    "\n",
    "    # use planet image to mask out regions of no data in the model inference\n",
    "    with rasterio.open(img) as src_ds:\n",
    "        nodata_mask = np.where(src_ds.read(1) == src_ds.profile['nodata'], 1, 0)\n",
    "        inference[nodata_mask==1] = 255\n",
    "        profile_copy = src_ds.profile\n",
    "        profile_copy.update({'count':1, 'dtype':np.uint8, 'nodata':255})\n",
    "\n",
    "        # write out model inference\n",
    "        with rasterio.open(f\"{classification.parent}/xgboost_classification.tif\", 'w', **profile_copy) as dst_ds:\n",
    "            dst_ds.write(inference.astype(np.uint8).reshape(1, *inference.shape))\n",
    "\n",
    "    print(f\"Completed inference for planet id {planet_id}\")\n",
    "    model_copy.unlink()\n",
    "\n",
    "print(\"Generating inferences for: \", planet_ids)\n",
    "\n",
    "_ = list(map(generate_inference, planet_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if UPDATE_DATABASE:\n",
    "    db_df = pd.read_csv('../data/new_validation_table.csv')\n",
    "    planet_ids = list(val_df['planet_id'])\n",
    "    inference_path = Path('../data/planet_images_cropped')\n",
    "    rf_classification_files = []\n",
    "    for id in planet_ids:\n",
    "        rf_classification_files.append(list((inference_path / id).glob('xgboost_classification.tif'))[0])\n",
    "\n",
    "    db_df['rf_classification_files'] = rf_classification_files\n",
    "    db_df.to_csv('../data/new_validation_table.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 64-bit ('expand-validation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d7560f7704abad77a83f3584272055367e63478e3eef1282c4e194da960c77d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
